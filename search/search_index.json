{"config":{"lang":["en"],"separator":"[\\s\\u200b\\-_,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to the SE NAI Workshop","text":"<p>This two-day, hands-on workshop is designed to equip you with the knowledge and skills to understand, deploy, and manage Nutanix Enterprise AI. You will gain a deep understanding of the NAI architecture, its core components, and how to leverage it to build and deploy generative AI applications.</p> <p></p> <p>Throughout this workshop, you will engage in a series of presentations, live demonstrations, and hands-on labs. You will learn how to:</p> <ul> <li>Understand the value proposition of Nutanix Enterprise AI</li> <li>Deploy and configure the Nutanix Kubernetes Platform (NKP)</li> <li>Install and manage Nutanix Enterprise AI</li> <li>Import and deploy Large Language Models (LLMs)</li> <li>Build and deploy a RAG-based chatbot using Flowise</li> <li>Prepare your own environment for delivering NAI workshops</li> </ul> <p>We are excited to have you here and look forward to a productive and engaging workshop.md engaging and productive workshop.</p>"},{"location":"day1/deploying-nkp-nai/","title":"Deploying NKP and NAI","text":"<p>This section will guide you through the process of deploying the Nutanix Kubernetes Platform (NKP) and Nutanix Enterprise AI (NAI) on a Hosted POC (HPOC). We will be following the best practices and procedures outlined in the NAI on NKP Tutorial.</p>"},{"location":"day1/deploying-nkp-nai/#nkp-high-level-cluster-design","title":"NKP High-Level Cluster Design","text":"<p>The <code>nkpdev</code> cluster will be hosting the LLM model serving endpoints and AI application stack. This cluster and will require a dedicated GPU node pool.</p>"},{"location":"day1/deploying-nkp-nai/#sizing-requirements","title":"Sizing Requirements","text":"<p>Below are the sizing requirements needed to successfully deploy NAI on a NKP Cluster (labeled as <code>nkpdev</code>) and subsequently deploying single LLM inferencing endpoint on NAI using the <code>meta-llama/Meta-Llama-3-8B-Instruct</code> LLM model.</p> Role No. of Nodes (VM) vCPU per Node Memory per Node Storage per Node Total vCPU Total Memory Control plane 3 4 16 GB 150 GB 12 48 GB Worker 4 8 32 GB 150 GB 32 128 GB GPU 1 16 40 GB 300 GB 16 40 GB Totals 8 60 216 GB"},{"location":"day1/deploying-nkp-nai/#gpu-resource-calculations","title":"GPU Resource Calculations","text":"<ul> <li>To host a 8b(illion) parameter model, multiply the parameter number by 2 to get minimum GPU memory requirments. e.g. 16GB of GPU memory is required for 8b parameter model.</li> </ul> <p>So in the case of the <code>meta-llama/Meta-Llama-3-8B-Instruct</code> model, you'll need a min. 16 GiB GPU vRAM available</p> <p>Below are additional sizing consideration \"Rule of Thumb\" for further calculating min. GPU node resources:</p> <ul> <li>For each GPU node will have 8 CPU cores, 24 GB of memory, and 300 GB of disk space.</li> <li>For each GPU attached to the node, add 16 GiB of memory.</li> <li>For each endpoint attached to the node, add 8 CPU cores.</li> <li>If a model needs multiple GPUs, ensure all GPUs are attached to the same worker node</li> <li>For resiliency, while running multiple instances of the same endpoint, ensure that the GPUs are on different worker nodes.</li> </ul> <p>Since we will be testing with the <code>meta-llama/Meta-Llama-3-8B-Instruct</code> HuggingFace model, we will require a GPU with a min. of 24 GiB GPU vRAM available to support this demo.</p>"},{"location":"day1/deploying-nkp-nai/#pre-requisites-for-nkp-deployment","title":"Pre-requisites for NKP Deployment","text":"<ol> <li>Existing Ubuntu Linux jumphost VM.</li> <li>Docker or Podman installed on the jumphost VM</li> <li>Nutanix PC is at least <code>2024.3</code></li> <li>Nutanix AOS is at least <code>6.8+</code>, <code>6.10</code></li> <li>Download and install <code>nkp</code> binary from Nutanix Portal</li> <li>Find and reserve 3 IPs for control plane and MetalLB access from AHV network</li> <li>Find GPU details from Nutanix cluster</li> <li>Create a base image to use with NKP nodes using <code>nkp</code> command</li> </ol>"},{"location":"day1/deploying-nkp-nai/#deployment-steps","title":"Deployment Steps","text":"<p>We will now walk through the high-level steps for deploying NKP and NAI. Detailed instructions will be provided in the hands-on lab.</p> <ol> <li>Install NKP Binaries: Download and install the <code>nkp</code> binary on your jumphost.</li> <li>Setup Docker on Jumphost: Ensure Docker is installed and running on your jumphost.</li> <li>Reserve Control Plane and MetalLB IP: Reserve the necessary IP addresses for the NKP control plane and MetalLB.</li> <li>Create Base Image for NKP: Create a base image for the NKP nodes using the <code>nkp</code> command.</li> <li>Create NKP Workload Cluster: Create the NKP workload cluster using the <code>nkp</code> command.</li> <li>Licensing: Generate and apply the NKP Pro license to the cluster.</li> <li>Add NKP GPU Workload Pool: Add a GPU workload pool to the cluster to support our AI workloads.</li> <li>Enable GPU Operator: Enable the GPU operator on the cluster to manage the GPU resources.</li> <li>Deploy NAI: Deploy Nutanix Enterprise AI on the NKP cluster using Helm.</li> <li>Test NAI: Test the NAI deployment by importing and deploying an LLM.</li> </ol>"},{"location":"day1/hands-on-lab/","title":"Hands-on Lab: Deploying NKP + NAI","text":""},{"location":"day1/hands-on-lab/#workshop-resources","title":"Workshop Resources","text":""},{"location":"day1/hands-on-lab/#presentation-slides","title":"Presentation Slides","text":"<ul> <li>05_SE_NAI_NAI_Hands-on_workshop_v1_02-06-2025.pptx</li> </ul>"},{"location":"day1/hands-on-lab/#deployment-summary-documents","title":"Deployment Summary Documents","text":"<ul> <li>nai_full_deployment_nai2.3_v1.docx</li> <li>nai_full_deployment_nai2.4_v2.docx</li> </ul> <p>This hands-on lab will guide you through the process of deploying the Nutanix Kubernetes Platform (NKP) and Nutanix Enterprise AI (NAI). We will be following the detailed instructions provided in the NAI on NKP Tutorial.</p>"},{"location":"day1/hands-on-lab/#lab-objectives","title":"Lab Objectives","text":"<p>By the end of this lab, you will have:</p> <ul> <li>Deployed a Nutanix Kubernetes Platform (NKP) cluster.</li> <li>Installed and configured Nutanix Enterprise AI (NAI) on the NKP cluster.</li> <li>Imported and deployed a Large Language Model (LLM).</li> <li>Created and tested an inference endpoint.</li> </ul>"},{"location":"day1/hands-on-lab/#lab-instructions","title":"Lab Instructions","text":"<p>We will be following the instructions in the NAI on NKP Tutorial. Please open the tutorial in a new browser tab and follow the steps outlined in the following sections:</p> <ol> <li> <p>Setup NKP Cluster: This section will guide you through the process of deploying the NKP cluster.</p> <ul> <li>Setup NKP Cluster</li> </ul> </li> <li> <p>Pre-requisites NAI: This section covers the prerequisites for installing NAI.</p> <ul> <li>Pre-requisites NAI</li> </ul> </li> <li> <p>Deploy NAI: This section will guide you through the process of deploying NAI on the NKP cluster.</p> <ul> <li>Deploy NAI</li> </ul> </li> <li> <p>Test NAI: This section will guide you through the process of testing the NAI deployment.</p> <ul> <li>Test NAI</li> </ul> </li> </ol>"},{"location":"day1/hands-on-lab/#key-commands-and-configurations","title":"Key Commands and Configurations","text":"<p>Throughout the lab, you will be using a variety of commands and configurations. Here are some of the key commands and configurations that you will be using:</p>"},{"location":"day1/hands-on-lab/#nkp-cluster-creation","title":"NKP Cluster Creation","text":"<pre><code># Create the NKP workload cluster\nnkp create cluster -f nkp-workload-cluster.yaml\n</code></pre>"},{"location":"day1/hands-on-lab/#nai-helm-chart-installation","title":"NAI Helm Chart Installation","text":"<pre><code># Add the Nutanix Helm repository\nhelm repo add nutanix https://nutanix.github.io/helm/\n\n# Update the Helm repository\nhelm repo update\n\n# Install the NAI Helm chart\nhelm install nai nutanix/nai -n nai --create-namespace -f values.yaml\n</code></pre>"},{"location":"day1/hands-on-lab/#llm-import","title":"LLM Import","text":"<pre><code># Import the Llama 3 8B Instruct model\nai-cli model import --name llama3-8b-instruct --from-huggingface meta-llama/Llama-3-8B-Instruct\n</code></pre>"},{"location":"day1/hands-on-lab/#inference-endpoint-creation","title":"Inference Endpoint Creation","text":"<pre><code># Create an inference endpoint for the Llama 3 model\nai-cli endpoint create --name llama3-8b-instruct --model llama3-8b-instruct --gpus 1\n</code></pre> <p>Good luck with the lab! Ask your instructor if you have any questions.</p>"},{"location":"day1/nai-call-flow/","title":"Typical NAI Call Flow","text":"<p>This section provides a step-by-step guide to using Nutanix Enterprise AI, from logging in to deploying and testing a Large Language Model (LLM). This workflow is based on the NAI demo video and will be the basis for our hands-on lab.</p>"},{"location":"day1/nai-call-flow/#1-logging-in-and-dashboard-overview","title":"1. Logging In and Dashboard Overview","text":"<p>First, connect to your corporate VPN and navigate to the Nutanix Enterprise AI portal to log in.</p> <ul> <li>URL: <code>ai.nutanix.com</code></li> </ul> <p></p> <p>Once logged in, you will see the main Dashboard. This provides a comprehensive overview of your AI environment, including: *   Endpoints Summary: Shows the status of your deployed models (Active, Failed, Pending). *   Infrastructure Summary: Displays the health and resource utilization (Memory, CPU, Disk, GPUs) of the underlying Kubernetes cluster. *   API Requests Summary &amp; Trends: Tracks the number of successful and failed API calls to your models over time.</p>"},{"location":"day1/nai-call-flow/#2-monitoring-infrastructure-health","title":"2. Monitoring Infrastructure Health","text":"<p>For a deeper look into your resource usage, you can inspect the infrastructure details. This is especially useful for administrators who need to monitor GPU performance and overall cluster health.</p> <ol> <li>On the main Dashboard, under the Infrastructure Summary pane, click View Usage Details.</li> <li>This view shows historical data for memory and CPU usage. To see GPU-specific metrics, select a GPU worker node from the Cluster dropdown menu.</li> <li>You can now monitor the GPU Utilization and GPU Memory Usage for the selected node.</li> </ol>"},{"location":"day1/nai-call-flow/#3-importing-a-language-model-llm","title":"3. Importing a Language Model (LLM)","text":"<p>Nutanix Enterprise AI simplifies the process of bringing models into your environment. You can import validated models directly from popular hubs like Hugging Face.</p> <ol> <li>From the left-hand navigation menu, click on Models.</li> <li>Click the Import Models button.</li> <li>Select From Hugging Face Model Hub. A list of validated models that have been tested to run on Nutanix Enterprise AI will appear.</li> <li>Choose a model from the list.</li> <li>In the pop-up window, provide a Model Instance Name.</li> <li>Click Import. The model's status will change to <code>Pending</code> as it is downloaded and prepared in the background.</li> </ol>"},{"location":"day1/nai-call-flow/#4-creating-and-deploying-a-model-endpoint","title":"4. Creating and Deploying a Model Endpoint","text":"<p>Once a model is imported, you need to create an endpoint to make it accessible for applications to use. The endpoint is the live, running instance of your model.</p> <ol> <li>Navigate to the Endpoints tab from the left-hand menu.</li> <li>Click the Create Endpoint button.</li> <li>Fill out the required details:<ul> <li>Endpoint Name: A unique name for your deployment.</li> <li>Model Instance Name: Select the model you just imported from the dropdown list.</li> <li>Number of GPUs (Per Instance): Assign the number of GPUs for the model to use.</li> <li>GPU Card: Select the type of GPU card available in your cluster.</li> <li>Number of Instances: Define how many copies of the model should run for high availability.</li> </ul> </li> <li>Click Create. The endpoint status will be <code>Pending</code> while the resources are allocated and the model is deployed.</li> </ol>"},{"location":"day1/nai-call-flow/#5-testing-the-endpoint-and-creating-an-api-key","title":"5. Testing the Endpoint and Creating an API Key","text":"<p>After the endpoint becomes <code>Active</code>, you can test it directly from the UI and create API keys for developers to use in their applications.</p> <p>To Test the Endpoint: 1. Click on your active endpoint from the list. 2. In the endpoint overview screen, click the Test button at the top. 3. Select a sample request or write a custom one, then click Test to see the model's response in real-time.</p> <p>To Create an API Key: 1. Navigate to the Endpoints tab and select the API Keys sub-tab. 2. Click Create API Key. 3. Give the key a name (e.g., <code>demo-app-key</code>) and select the endpoint(s) it should have access to. 4. Click Create. Copy the generated API key and store it securely, as it will not be shown again.</p>"},{"location":"day1/nai-call-flow/#6-integrating-with-an-application-example-ai-chatbot","title":"6. Integrating with an Application (Example: AI Chatbot)","text":"<p>Nutanix Enterprise AI is compatible with the OpenAI API standard, making it easy to switch your existing applications from public cloud services to your private on-prem models with minimal code changes.</p> <p>To reconfigure an existing application, you only need to update three pieces of information: 1.  API Endpoint URL: Change this from the public URL (e.g., <code>api.openai.com</code>) to your Nutanix AI endpoint URL. 2.  Model Name: Change this to the name of your deployed endpoint (e.g., <code>vllm-llama-3-1</code>). 3.  API Key: Replace the existing key with the new one you generated on the Nutanix AI platform.</p>"},{"location":"day1/nai-call-flow/#7-advanced-use-case-creating-an-ai-agent","title":"7. Advanced Use Case: Creating an AI Agent","text":"<p>You can transform your LLM into a powerful AI agent capable of performing tasks by providing specific instructions in the System Message. This allows the model to interact with other tools and APIs.</p> <p>In this example, the chatbot is instructed to act as a tool for creating virtual machines. When a user makes a request in natural language, the model extracts the necessary information, fills in the blanks with default values, and generates a structured JSON output. This JSON can then be passed to the Nutanix Prism Central API to automate the VM creation process.</p>"},{"location":"day1/nai-introduction/","title":"Introduction to Nutanix Enterprise AI","text":"<p>This section provides access to comprehensive presentation materials covering Nutanix Enterprise AI solutions. The presentations cover various aspects of AI implementation, from basic introductions to specialized solutions for different market segments.</p>"},{"location":"day1/nai-introduction/#presentation-materials","title":"Presentation Materials","text":""},{"location":"day1/nai-introduction/#1-nai-introduction-and-demo","title":"1. NAI Introduction and Demo","text":"<p>01_SE_NAI-Introduction_and_demo_v1_02-06-2025.pptx</p> <p>Download Presentation</p> <p>This presentation provides a comprehensive introduction to Nutanix Enterprise AI, including key concepts, value propositions, and live demonstration of the platform capabilities.</p>"},{"location":"day1/nai-introduction/#2-nutanix-ai-solution-for-msp","title":"2. Nutanix AI Solution for MSP","text":"<p>02_Nutanix AI Solution for MSP_v1_24-06-2025.pptx</p> <p>Download Presentation</p> <p>This presentation focuses on how Managed Service Providers (MSPs) can leverage Nutanix AI solutions to deliver AI services to their customers.</p>"},{"location":"day1/nai-introduction/#3-university-solution-overview","title":"3. University Solution Overview","text":"<p>03_university_solution_overview_v1.pptx</p> <p>Download Presentation</p> <p>This presentation outlines Nutanix AI solutions specifically designed for educational institutions and university environments.</p>"},{"location":"day1/nai-introduction/#4-nai-agents-presentation","title":"4. NAI Agents Presentation","text":"<p>04_SE_NAI_agents_presentation_v1_02-06-2025.pptx</p> <p>Download Presentation</p> <p>This presentation covers AI agents and their implementation within the Nutanix Enterprise AI platform.</p> <p>Note: These presentations are hosted on Nutanix SharePoint and require appropriate access permissions to view.</p>"},{"location":"day1/welcome/","title":"Welcome to Day 1","text":"<p>Welcome to the first day of the Nutanix Enterprise AI Trainer-Enablement Workshop! Today, we will focus on building a strong foundation in Nutanix Enterprise AI. We will cover the core concepts, architecture, and deployment of the NAI stack.</p>"},{"location":"day1/welcome/#day-1-agenda","title":"Day 1 Agenda","text":"<ul> <li>Welcome and Workshop check-in: We'll start with a brief welcome and check-in to ensure everyone is ready for the day.</li> <li>Workshop Overview: We'll provide a high-level overview of the workshop, including the agenda, goals, and learning objectives.</li> <li>Introduction to NAI: We'll dive into the positioning, value proposition, competitive landscape, and sizing of Nutanix Enterprise AI.</li> <li>Typical NAI Call Flow: You'll learn about the typical customer engagement process and how to effectively present the NAI solution.</li> <li>Deploying NKP and NAI: We'll walk through the steps to deploy the Nutanix Kubernetes Platform and install Nutanix Enterprise AI on a Hosted POC (HPOC).</li> <li>Hands-on Lab: You'll get your hands dirty by deploying NKP and NAI in a guided lab environment.</li> </ul> <p>Let's get started!</p>"},{"location":"day2/deploy-chatbot/","title":"Deploy Your First Chatbot","text":"<p>In this section, you will deploy your first chatbot using the skills you have acquired so far. We will be using the Nutanix AI Demo GitHub repository to deploy a simple chatbot that integrates with the Nutanix Enterprise AI platform.</p>"},{"location":"day2/deploy-chatbot/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, please ensure that you have completed the following prerequisites:</p> <ul> <li>You have a running Nutanix Enterprise AI instance.</li> <li>You have imported and deployed a Large Language Model (LLM).</li> <li>You have created an inference endpoint for the LLM.</li> <li>You have an API key for the inference endpoint.</li> </ul>"},{"location":"day2/deploy-chatbot/#deployment-instructions","title":"Deployment Instructions","text":"<p>We will be following the instructions in the Nutanix AI Demo GitHub repository. Please open the repository in a new browser tab and follow the instructions for your operating system.</p>"},{"location":"day2/deploy-chatbot/#macos-instructions","title":"macOS Instructions","text":"<ol> <li>Open your terminal.</li> <li>Clone the repository:     <pre><code>git clone https://github.com/halsayed/nai-demo.git\n</code></pre></li> <li>Navigate to the project directory:     <pre><code>cd nai-demo\n</code></pre></li> <li>Build the Docker containers:     <pre><code>docker compose build\n</code></pre></li> <li>Start the application:     <pre><code>docker compose up\n</code></pre></li> <li>Open your web browser and navigate to:     <pre><code>http://localhost:8000\n</code></pre></li> <li>To stop the application, press <code>Control + C</code>.</li> </ol>"},{"location":"day2/deploy-chatbot/#linux-ubuntu-instructions","title":"Linux (Ubuntu) Instructions","text":"<ul> <li>Follow the macOS instructions to run the application.</li> </ul>"},{"location":"day2/deploy-chatbot/#windows-instructions","title":"Windows Instructions","text":"<ol> <li>Install WSL from an admin command/Powershell prompt by running <code>wsl --install</code>.</li> <li>Once installed and rebooted, open WSL and follow the instructions for installing Docker on Linux.</li> <li>Exit WSL to allow the group change to take effect.</li> <li>Open WSL and continue with the macOS instructions to run the application.</li> </ol>"},{"location":"day2/deploy-chatbot/#configuration","title":"Configuration","text":"<p>Once the application is running, you will need to configure it to use your Nutanix Enterprise AI instance. To do this, you will need to update the following environment variables in the <code>docker-compose.yaml</code> file:</p> <ul> <li><code>NAI_API_ENDPOINT</code>: The URL of your Nutanix Enterprise AI inference endpoint.</li> <li><code>NAI_MODEL_NAME</code>: The name of your deployed LLM.</li> <li><code>NAI_API_KEY</code>: Your Nutanix Enterprise AI API key.</li> </ul> <p>Once you have updated these environment variables, you will need to restart the application for the changes to take effect.</p>"},{"location":"day2/deploy-chatbot/#testing","title":"Testing","text":"<p>Once the application is running and configured, you can test it by opening your web browser and navigating to <code>http://localhost:8000</code>. You should see a simple chatbot interface. You can now start chatting with your chatbot!</p>"},{"location":"day2/flowise-introduction/","title":"Flowise.ai Introduction","text":"<p>In this section, we will introduce you to Flowise, a low-code/no-code tool for building AI-powered workflows and applications. We will be using Flowise to build a Retrieval-Augmented Generation (RAG) chatbot that can answer questions about your own private data.</p>"},{"location":"day2/flowise-introduction/#what-is-flowise","title":"What is Flowise?","text":"<p>Flowise is an open-source, low-code tool that enables developers to build customized LLM orchestration flows and AI agents on top of LangChain and/or LlamaIndex. LangChain and LlamaIndex are frameworks for simplifying the development of AI applications.</p>"},{"location":"day2/flowise-introduction/#how-does-flowise-work","title":"How does Flowise work?","text":"<p>Flowise provides a visual interface for building AI-powered workflows. You can drag and drop different nodes onto a canvas and connect them to create a workflow. Each node represents a specific function, such as a language model, a document loader, or a vector store.</p>"},{"location":"day2/flowise-introduction/#chatbot-flow","title":"Chatbot Flow","text":"<p>The following diagram illustrates the flow of a chatbot built with Flowise:</p> <p></p> <ol> <li>Ask Question: The user asks a question to the chatbot.</li> <li>Send Prompt to Inference API: The chatbot sends the user's question to the Nutanix Enterprise AI inference endpoint.</li> <li>Get Answer: The chatbot receives a response from the inference endpoint and displays it to the user.</li> </ol>"},{"location":"day2/flowise-introduction/#steps-to-create-a-chatbot","title":"Steps to Create a Chatbot","text":"<p>In the next section, we will walk you through the process of creating a chatbot with Flowise. Here are the high-level steps:</p> <ol> <li>Gather information from Nutanix Enterprise AI: You will need to gather the endpoint details from your Nutanix Enterprise AI instance.</li> <li>Create a Chatflow in Flowise: You will create a new chatflow in the Flowise application.</li> <li>Add, configure, and connect nodes in the chatflow: You will add and configure the necessary nodes to create your chatbot.</li> <li>Test the chatflow: You will test your chatbot to ensure that it is working correctly.</li> </ol>"},{"location":"day2/llm-concepts/","title":"LLM Concepts Recap","text":"<p>This section provides a recap of key Large Language Model (LLM) concepts. This will serve as a refresher before we dive into building our own AI-powered applications.</p>"},{"location":"day2/llm-concepts/#what-is-an-llm","title":"What is an LLM?","text":"<p>A Large Language Model (LLM) is a type of artificial intelligence (AI) that is trained on massive amounts of text data to understand and generate human-like text. LLMs are the foundation of many generative AI applications, including chatbots, content creation tools, and code assistants.</p>"},{"location":"day2/llm-concepts/#why-llms-now","title":"Why LLMs Now?","text":"<p>The recent explosion in the popularity of LLMs is due to a combination of factors:</p> <ul> <li>Attention is All You Need: The development of the Transformer architecture, which is based on the attention mechanism, has been a key enabler of LLMs.</li> <li>Large Datasets: The availability of massive datasets, such as the Common Crawl, has provided the necessary training data for LLMs.</li> <li>Compute Power: The availability of powerful GPUs has made it possible to train these large models in a reasonable amount of time.</li> </ul>"},{"location":"day2/llm-concepts/#popular-open-source-llms","title":"Popular Open-Source LLMs","text":"<p>There are a number of popular open-source LLMs available, including:</p> <ul> <li>Mistral: A family of high-performance LLMs from Mistral AI.</li> <li>LLaMA: A family of LLMs from Meta AI.</li> <li>Gemma: A family of lightweight, state-of-the-art open models from Google.</li> <li>Qwen: A series of LLMs from Alibaba Cloud.</li> <li>Falcon: A family of LLMs from the Technology Innovation Institute (TII).</li> <li>Phi: A family of small, powerful LLMs from Microsoft.</li> </ul>"},{"location":"day2/llm-concepts/#model-modes","title":"Model Modes","text":"<p>LLMs can be fine-tuned for a variety of tasks. Here are some of the most common model modes:</p> <ul> <li>Text Completion: This is the most basic mode, where the model predicts the next word in a sequence.</li> <li>Chat: This mode is fine-tuned for human conversation and is suitable for building chatbots.</li> <li>Instruct: This mode is fine-tuned to follow instructions and perform specific tasks.</li> <li>Code: This mode is fine-tuned on a large codebase and is suitable for coding assistants and programming language translation.</li> <li>Guard: This mode is fine-tuned to classify content safety.</li> </ul>"},{"location":"day2/llm-concepts/#tokens-vs-words","title":"Tokens vs. Words","text":"<p>LLMs process text in chunks called tokens. A token can be a word, a part of a word, or a punctuation mark. The number of tokens in a given text is usually greater than the number of words.</p>"},{"location":"day2/llm-concepts/#benchmarking","title":"Benchmarking","text":"<p>LLMs are benchmarked on a variety of tasks to evaluate their performance. Some of the most common benchmarks include:</p> <ul> <li>MMLU: A massive multitask language understanding benchmark.</li> <li>HellaSwag: A benchmark for commonsense reasoning.</li> <li>TruthfulQA: A benchmark for evaluating the truthfulness of LLMs.</li> </ul>"},{"location":"day2/llm-concepts/#inferencing-for-infra-teams","title":"Inferencing for Infra Teams","text":"<p>When deploying LLMs, infrastructure teams need to consider the following:</p> <ul> <li>Throughput: The number of queries per second that the model can handle.</li> <li>Latency: The time it takes for the model to respond to a query.</li> <li>Memory: The amount of memory required to run the model.</li> </ul>"},{"location":"day2/llm-concepts/#reducing-vram","title":"Reducing VRAM","text":"<p>There are a number of techniques for reducing the VRAM footprint of LLMs, including:</p> <ul> <li>Sharding: Splitting the model across multiple GPUs.</li> <li>Quantization: Reducing the precision of the model's weights.</li> </ul>"},{"location":"day2/llm-concepts/#fine-tuning","title":"Fine-Tuning","text":"<p>Fine-tuning is the process of adapting a pre-trained LLM to a specific task. There are a number of fine-tuning techniques, including:</p> <ul> <li>Full Training: Retraining the entire model on a new dataset.</li> <li>Transfer Learning: Retraining only the last few layers of the model.</li> <li>PEFT: Parameter-Efficient Fine-Tuning, which involves adding a small number of new parameters to the model and only training those parameters.</li> <li>RAG: Retrieval-Augmented Generation, which involves retrieving relevant documents from a knowledge base and using them to augment the model's input.</li> </ul>"},{"location":"day2/llm-concepts/#embeddings","title":"Embeddings","text":"<p>Embeddings are numerical representations of words or phrases. They are used to capture the semantic meaning of text and are a key component of many LLM applications.</p>"},{"location":"day2/llm-concepts/#private-chatgpt-with-docs","title":"Private ChatGPT with Docs","text":"<p>A private ChatGPT with docs is a chatbot that is trained on your own private data. This allows you to build a chatbot that can answer questions about your specific domain.</p>"},{"location":"day2/llm-concepts/#llm-safety","title":"LLM Safety","text":"<p>LLM safety is a critical concern. There are a number of challenges with LLMs in the real world, including:</p> <ul> <li>Prompt Jailbreaking: Users can try to bypass the model's safety guardrails by crafting malicious prompts.</li> <li>Hallucinations: LLMs can sometimes generate text that is factually incorrect or nonsensical.</li> </ul> <p>There are a number of techniques for mitigating these risks, including:</p> <ul> <li>Advanced pattern recognition: Using advanced pattern recognition techniques to detect and block malicious prompts.</li> <li>Prompt techniques: Using prompt engineering techniques to guide the model's output.</li> <li>Improving training data quality: Using high-quality training data to reduce the likelihood of hallucinations.</li> <li>Cross-referencing with techniques like RAG: Using RAG to ground the model's output in factual information.</li> <li>Human feedback in the loop: Using human feedback to fine-tune the model and improve its safety.</li> </ul>"},{"location":"day2/n8n-introduction/","title":"n8n Introduction","text":"<p>In this section, we will introduce you to n8n, a low-code/no-code workflow automation tool. We will explore how you can use n8n to build powerful workflows that integrate with Nutanix Enterprise AI.</p>"},{"location":"day2/n8n-introduction/#what-is-n8n","title":"What is n8n?","text":"<p>n8n is an open-source, self-hostable workflow automation tool that allows you to connect different applications and services to automate tasks. It provides a visual interface for building workflows, making it easy to create complex automations without writing any code.</p>"},{"location":"day2/n8n-introduction/#how-can-n8n-be-used-with-nutanix-enterprise-ai","title":"How can n8n be used with Nutanix Enterprise AI?","text":"<p>You can use n8n to build workflows that leverage the power of Nutanix Enterprise AI. For example, you can create a workflow that automatically:</p> <ul> <li>Analyzes customer feedback from a survey and sends a summary to your team in Slack.</li> <li>Generates a personalized email response to a customer inquiry.</li> <li>Creates a new virtual machine in Nutanix Prism Central based on a natural language request.</li> </ul>"},{"location":"day2/n8n-introduction/#workflow-example","title":"Workflow Example","text":"<p>The following diagram illustrates an example of an n8n workflow that integrates with Nutanix Enterprise AI:</p> <pre><code>graph TD\n    A[Webhook] --&gt; B{Nutanix Enterprise AI};\n    B --&gt; C{Slack};</code></pre> <ol> <li>Webhook: The workflow is triggered by a webhook, which is a a web-based event notification.</li> <li>Nutanix Enterprise AI: The workflow sends a prompt to the Nutanix Enterprise AI inference endpoint.</li> <li>Slack: The workflow sends the response from the inference endpoint to a Slack channel.</li> </ol>"},{"location":"day2/n8n-introduction/#building-a-workflow","title":"Building a Workflow","text":"<p>In the next section, we will walk you through the process of building a workflow with n8n. Here are the high-level steps:</p> <ol> <li>Create a new workflow: You will create a new workflow in the n8n application.</li> <li>Add nodes to the workflow: You will add and configure the necessary nodes to create your workflow.</li> <li>Test the workflow: You will test your workflow to ensure that it is working correctly.</li> </ol>"},{"location":"day2/preparing-hpoc/","title":"Preparing HPOC for NAI Workshop","text":"<p>This section provides a guide for preparing a Hosted POC (HPOC) for delivering the Nutanix Enterprise AI workshop. This will ensure that you have a consistent and reliable environment for your workshop participants.</p>"},{"location":"day2/preparing-hpoc/#hpoc-configuration","title":"HPOC Configuration","text":"<p>To prepare your HPOC for the NAI workshop, you will need to perform the following tasks:</p> <ol> <li>Create a new project: Create a new project in Nutanix Prism Central for the workshop.</li> <li>Create a new user: Create a new user in Nutanix Prism Central for the workshop participants.</li> <li>Assign the user to the project: Assign the user to the project with the appropriate permissions.</li> <li>Create a new virtual machine: Create a new virtual machine for the jumphost.</li> <li>Install Docker on the jumphost: Install Docker on the jumphost.</li> <li>Install the <code>nkp</code> binary on the jumphost: Install the <code>nkp</code> binary on the jumphost.</li> <li>Reserve IP addresses: Reserve the necessary IP addresses for the NKP control plane and MetalLB.</li> <li>Create a base image for NKP: Create a base image for the NKP nodes.</li> </ol>"},{"location":"day2/preparing-hpoc/#workshop-cleanup","title":"Workshop Cleanup","text":"<p>After the workshop is complete, you will need to clean up the HPOC environment. This will ensure that the environment is ready for the next workshop.</p> <ol> <li>Delete the NKP cluster: Delete the NKP cluster.</li> <li>Delete the base image: Delete the base image.</li> <li>Delete the jumphost: Delete the jumphost.</li> <li>Delete the project: Delete the project.</li> <li>Delete the user: Delete the user.</li> </ol> <p>By following these steps, you can ensure that you have a consistent and reliable environment for delivering the Nutanix Enterprise AI workshop.</p>"},{"location":"day2/welcome/","title":"Welcome to Day 2","text":"<p>Welcome to the second day of the Nutanix Enterprise AI Trainer-Enablement Workshop! Today, we will build on the foundation we established on Day 1 and focus on building and delivering AI-powered applications. We'll also cover the practical aspects of preparing for and delivering the customer-facing workshop.</p>"},{"location":"day2/welcome/#day-2-agenda","title":"Day 2 Agenda","text":"<ul> <li>Welcome to Day 2: We'll start with a brief welcome and a recap of what we covered on Day 1.</li> <li>LLM Concepts Recap: We'll have a quick refresher on key Large Language Model concepts to ensure everyone is on the same page.</li> <li>Deploy Your First Chatbot: You'll learn how to deploy a simple chatbot using the skills you've acquired.</li> <li>Flowise.ai and n8n Introduction: We'll introduce you to low-code/no-code tools for building AI-powered workflows and applications.</li> <li>Preparing HPOC for NAI Workshop: You'll get hands-on experience preparing the HPOC environment for delivering the customer-facing workshop.</li> </ul> <p>Let's get started!</p>"},{"location":"overview/introduction/","title":"Workshop Introduction","text":"<p>Welcome to the Nutanix Enterprise AI Trainer-Enablement Workshop! This two-day, face-to-face workshop is designed to provide pre-sales engineers with the technical expertise and facilitation skills necessary to conduct Nutanix's customer-facing GenAI workshops.</p>"},{"location":"overview/introduction/#workshop-goals","title":"Workshop Goals","text":"<p>Upon completion of this workshop, you will be able to:</p> <ul> <li>Articulate the value proposition of Nutanix Enterprise AI (NAI).</li> <li>Understand the competitive landscape and sizing considerations for NAI.</li> <li>Deploy and configure the Nutanix Kubernetes Platform (NKP) for AI workloads.</li> <li>Install and manage Nutanix Enterprise AI using Helm.</li> <li>Import and deploy open-source Large Language Models (LLMs).</li> <li>Create and expose inference endpoints for AI applications.</li> <li>Build a Retrieval-Augmented Generation (RAG) chatbot using Flowise.</li> <li>Prepare and deliver the one-day customer-facing NAI workshop.</li> </ul>"},{"location":"overview/introduction/#workshop-agenda","title":"Workshop Agenda","text":""},{"location":"overview/introduction/#day-1-the-foundation","title":"Day 1: The Foundation","text":"<p>Day 1 focuses on building a strong foundation in Nutanix Enterprise AI. We will cover the core concepts, architecture, and deployment of the NAI stack.</p> <ul> <li>Introduction to NAI: We'll start with an in-depth look at the positioning, value proposition, competitive landscape, and sizing of Nutanix Enterprise AI.</li> <li>NAI Call Flow: You'll learn about the typical customer engagement process and how to effectively present the NAI solution.</li> <li>Deploying NKP and NAI: We'll walk through the steps to deploy the Nutanix Kubernetes Platform and install Nutanix Enterprise AI on a Hosted POC (HPOC).</li> <li>Hands-on Lab: You'll get your hands dirty by deploying NKP and NAI in a guided lab environment.</li> </ul>"},{"location":"overview/introduction/#day-2-building-and-delivering","title":"Day 2: Building and Delivering","text":"<p>Day 2 is all about building on the foundation and preparing you to deliver the workshop to customers. We'll dive into building AI applications and the practical aspects of workshop delivery.</p> <ul> <li>LLM Concepts Recap: We'll start with a refresher on key Large Language Model concepts to ensure everyone is on the same page.</li> <li>Deploy Your First Chatbot: You'll learn how to deploy a simple chatbot using the skills you've acquired.</li> <li>Flowise.ai and n8n Introduction: We'll introduce you to low-code/no-code tools for building AI-powered workflows and applications.</li> <li>Preparing HPOC for NAI Workshop: You'll get hands-on experience preparing the HPOC environment for delivering the customer-facing workshop.</li> </ul>"},{"location":"overview/lab-environment/","title":"Lab Environment","text":"<p>This workshop will be conducted in a dedicated lab environment designed to provide you with a hands-on experience with Nutanix Enterprise AI. The lab environment consists of the following components:</p>"},{"location":"overview/lab-environment/#lab-architecture","title":"Lab Architecture","text":"<p>The lab environment is designed to mirror a real-world deployment of Nutanix Enterprise AI. The following diagram illustrates the high-level architecture of our lab:</p> <pre><code>graph TD\n    A[Your Laptop] --&gt; B{HPOC or Corporate VPN};\n    B --&gt; C[Shared GPU Pre-Configured Cluster];\n    B --&gt; D[Lab non-GPU Clusters];\n    C --&gt; F[https://demo.lab.ntnx.pro]\n    D --&gt; G[DM3]\n    G --&gt; H[DM3-POC100 https://10.54.28.7:9440/]\n    G --&gt; I[DM3-POC101 https://10.54.29.7:9440/]\n    D --&gt; J[PHX]\n    J --&gt; K[PHX-POC169 https://10.42.169.7:9440/]\n    K --&gt; L[PHX-POC287 https://10.38.59.7:9440/]\n    J --&gt; M[PHX-POC252 https://10.38.252.7:9440/]\n    M --&gt; N[PHX-POC255 https://10.42.153.7:9440/]</code></pre>"},{"location":"overview/lab-environment/#poc-details","title":"POC Details","text":"<p>For detailed information about the lab environment, please refer to the POC detail sheet:</p> <p>POC Detail Sheet</p>"},{"location":"overview/lab-environment/#default-credentials","title":"Default Credentials","text":"<p>The default credentials for the GPU clusters are:</p> <ul> <li>Username: <code>admin</code></li> <li>Password: <code>nx2Tech911!</code></li> </ul>"},{"location":"overview/prerequisites/","title":"Workshop Prerequisites","text":"<p>To ensure a successful and productive workshop experience, it is essential that you meet the following prerequisites and complete the pre-work assignments before attending.</p>"},{"location":"overview/prerequisites/#technical-prerequisites","title":"Technical Prerequisites","text":"<p>Participants are expected to have a solid understanding of the following technical concepts:</p> <ul> <li> <p>Kubernetes: A fundamental understanding of Kubernetes architecture is required, including concepts such as pods, services, deployments, and namespaces. You should be comfortable using the <code>kubectl</code> command-line interface (CLI) to interact with a Kubernetes cluster.</p> </li> <li> <p>Docker: Docker must be installed and running on your local machine. You should have basic knowledge of running local containers, building Docker images, and using Docker Compose.</p> </li> <li> <p>REST APIs and JSON: Familiarity with RESTful APIs and the JSON data format is essential. You should understand how to make API requests and interpret JSON responses.</p> </li> <li> <p>GenAI Concepts: A basic awareness of Generative AI (GenAI) concepts is expected. You should be familiar with terms such as Large Language Model (LLM), Retrieval-Augmented Generation (RAG), tokens, and prompt engineering.</p> </li> </ul>"},{"location":"overview/prerequisites/#pre-work-assignments","title":"Pre-work Assignments","text":"<p>Please complete the following pre-work assignments before the workshop:</p> <ol> <li> <p>Watch the NAI Pitch: Gain a high-level understanding of the Nutanix Enterprise AI value proposition by watching the official NAI pitch video. You can access the video at the following link:     NAI Pitch</p> </li> <li> <p>Getting Started with Enterprise AI: Watch the introductory video on YouTube to familiarize yourself with the basics of Nutanix Enterprise AI.     Getting Started with Enterprise AI</p> </li> <li> <p>Review the NAI Demo: Watch the NAI Demo video to see the platform in action.</p> </li> <li> <p>Introduction to LLMs: Review the \"Introduction to LLMs\" presentation to build a foundational understanding of Large Language Models.</p> </li> <li> <p>Deploy a Basic Chatbot: Follow the instructions in the provided GitHub repository to deploy a basic chatbot on your local machine. This will give you hands-on experience with the technologies we will be using in the workshop.     Nutanix AI Demo GitHub Repository</p> </li> <li> <p>Experiment with Prompts: Spend some time experimenting with different prompts to understand how they influence the output of Large Language Models.</p> </li> <li> <p>Create Accounts:</p> <ul> <li>Create a free account on Docker Hub.</li> <li>Create a free account on Hugging Face.</li> </ul> </li> <li> <p>Accept Model Usage Policies: On the Hugging Face website, accept the usage policies for the following models:</p> <ul> <li>meta-llama/Llama-3.2-1B-Instruct</li> <li>meta-llama/Llama-3.2-3B-Instruct</li> </ul> </li> <li> <p>Bonus: Skim Additional Workshops: For extra preparation, you can skim through the following workshop materials:</p> <ul> <li>NAI on NKP Workshop</li> <li>Nutanix .Next 2025 AI Labs</li> <li>AI Engineer Roadmap</li> </ul> </li> </ol>"},{"location":"resources/additional-reading/","title":"Additional Reading","text":"<p>This page provides a collection of additional reading materials that will help you deepen your understanding of the topics covered in the workshop.</p>"},{"location":"resources/additional-reading/#nutanix-enterprise-ai","title":"Nutanix Enterprise AI","text":"<ul> <li>Nutanix Enterprise AI Documentation - Official documentation for Nutanix Enterprise AI.</li> <li>Nutanix Enterprise AI Best Practices - Best practices for deploying and managing Nutanix Enterprise AI.</li> </ul>"},{"location":"resources/additional-reading/#kubernetes-and-nkp","title":"Kubernetes and NKP","text":"<ul> <li>Kubernetes Documentation - Official Kubernetes documentation.</li> <li>Nutanix Kubernetes Platform Documentation - Official documentation for Nutanix Kubernetes Platform.</li> </ul>"},{"location":"resources/additional-reading/#large-language-models","title":"Large Language Models","text":"<ul> <li>The Illustrated GPT-2 - Visual explanation of the GPT-2 architecture.</li> <li>Understanding Large Language Models - Comprehensive guide to understanding LLMs.</li> <li>Prompt Engineering Guide - Guide to prompt engineering techniques.</li> </ul>"},{"location":"resources/additional-reading/#ai-application-development","title":"AI Application Development","text":"<ul> <li>LangChain Documentation - Official documentation for LangChain.</li> <li>LlamaIndex Documentation - Official documentation for LlamaIndex.</li> <li>Building LLM Applications - Course on building LLM applications.</li> </ul>"},{"location":"resources/additional-reading/#retrieval-augmented-generation-rag","title":"Retrieval-Augmented Generation (RAG)","text":"<ul> <li>Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks - The original RAG paper.</li> <li>RAG vs Fine-tuning - Comparison of RAG and fine-tuning approaches.</li> </ul>"},{"location":"resources/additional-reading/#ai-ethics-and-safety","title":"AI Ethics and Safety","text":"<ul> <li>AI Safety Fundamentals - Course on AI safety fundamentals.</li> <li>Responsible AI Practices - Google's responsible AI practices.</li> <li>AI Ethics Guidelines - Partnership on AI ethics guidelines.</li> </ul>"},{"location":"resources/external-links/","title":"External Links","text":"<p>This page provides a collection of external links and resources that are relevant to the Nutanix Enterprise AI workshop.</p>"},{"location":"resources/external-links/#official-nutanix-resources","title":"Official Nutanix Resources","text":"<ul> <li>Nutanix Portal - Official Nutanix portal for downloads and documentation.</li> <li>Nutanix Developer Portal - Developer resources and API documentation.</li> <li>Nutanix Community - Community forum for Nutanix users and developers.</li> </ul>"},{"location":"resources/external-links/#workshop-resources","title":"Workshop Resources","text":"<ul> <li>NAI on NKP Tutorial - Comprehensive tutorial for deploying NAI on NKP.</li> <li>Nutanix .Next 2025 AI Labs - Hands-on labs for Nutanix Enterprise AI.</li> <li>Nutanix AI Demo GitHub Repository - Demo application for Nutanix AI.</li> </ul>"},{"location":"resources/external-links/#ai-and-machine-learning-resources","title":"AI and Machine Learning Resources","text":"<ul> <li>Hugging Face - Platform for sharing and discovering machine learning models.</li> <li>LangChain - Framework for developing applications powered by language models.</li> <li>LlamaIndex - Data framework for LLM applications.</li> <li>AI Engineer Roadmap - Roadmap for becoming an AI engineer.</li> </ul>"},{"location":"resources/external-links/#tools-and-platforms","title":"Tools and Platforms","text":"<ul> <li>Flowise - Open-source low-code tool for building LLM flows.</li> <li>n8n - Workflow automation tool.</li> <li>Docker - Containerization platform.</li> <li>Kubernetes - Container orchestration platform.</li> </ul>"},{"location":"resources/external-links/#educational-resources","title":"Educational Resources","text":"<ul> <li>Getting Started with Enterprise AI - Introductory video on Nutanix Enterprise AI.</li> <li>Attention is All You Need - The original Transformer paper.</li> <li>The Illustrated Transformer - Visual explanation of the Transformer architecture.</li> </ul>"},{"location":"resources/troubleshooting/","title":"Troubleshooting","text":"<p>This page provides solutions to common issues that you may encounter during the workshop.</p>"},{"location":"resources/troubleshooting/#nkp-deployment-issues","title":"NKP Deployment Issues","text":""},{"location":"resources/troubleshooting/#issue-nkp-cluster-creation-fails","title":"Issue: NKP cluster creation fails","text":"<p>Symptoms: The <code>nkp create cluster</code> command fails with an error.</p> <p>Solution:  1. Check that you have the correct permissions to create resources in the Nutanix cluster. 2. Verify that the base image has been created successfully. 3. Check that the IP addresses have been reserved correctly.</p>"},{"location":"resources/troubleshooting/#issue-gpu-node-is-not-recognized","title":"Issue: GPU node is not recognized","text":"<p>Symptoms: The GPU node is not showing up in the cluster or the GPU operator is not working.</p> <p>Solution: 1. Verify that the GPU node has been created with the correct GPU configuration. 2. Check that the GPU operator has been installed and is running. 3. Verify that the GPU drivers are installed on the node.</p>"},{"location":"resources/troubleshooting/#nai-deployment-issues","title":"NAI Deployment Issues","text":""},{"location":"resources/troubleshooting/#issue-nai-helm-chart-installation-fails","title":"Issue: NAI Helm chart installation fails","text":"<p>Symptoms: The <code>helm install</code> command fails with an error.</p> <p>Solution: 1. Check that the Helm repository has been added correctly. 2. Verify that the values.yaml file is configured correctly. 3. Check that the namespace exists or use the <code>--create-namespace</code> flag.</p>"},{"location":"resources/troubleshooting/#issue-model-import-fails","title":"Issue: Model import fails","text":"<p>Symptoms: The model import process fails or gets stuck.</p> <p>Solution: 1. Check that you have the correct permissions to access the Hugging Face model. 2. Verify that the model is supported by Nutanix Enterprise AI. 3. Check the network connectivity to the Hugging Face repository.</p>"},{"location":"resources/troubleshooting/#chatbot-deployment-issues","title":"Chatbot Deployment Issues","text":""},{"location":"resources/troubleshooting/#issue-docker-containers-fail-to-start","title":"Issue: Docker containers fail to start","text":"<p>Symptoms: The <code>docker compose up</code> command fails or the containers exit immediately.</p> <p>Solution: 1. Check that Docker is installed and running. 2. Verify that the docker-compose.yaml file is configured correctly. 3. Check the Docker logs for error messages.</p>"},{"location":"resources/troubleshooting/#issue-chatbot-cannot-connect-to-nai","title":"Issue: Chatbot cannot connect to NAI","text":"<p>Symptoms: The chatbot interface loads but cannot connect to the NAI endpoint.</p> <p>Solution: 1. Verify that the NAI endpoint URL is correct. 2. Check that the API key is valid and has the correct permissions. 3. Verify that the model name is correct.</p>"},{"location":"resources/troubleshooting/#general-issues","title":"General Issues","text":""},{"location":"resources/troubleshooting/#issue-network-connectivity-problems","title":"Issue: Network connectivity problems","text":"<p>Symptoms: Cannot access external resources or services.</p> <p>Solution: 1. Check the network configuration of the jumphost. 2. Verify that the firewall rules are configured correctly. 3. Check the DNS configuration.</p>"},{"location":"resources/troubleshooting/#issue-permission-denied-errors","title":"Issue: Permission denied errors","text":"<p>Symptoms: Commands fail with permission denied errors.</p> <p>Solution: 1. Check that you are running the commands with the correct user. 2. Verify that the user has the necessary permissions. 3. Use <code>sudo</code> if necessary.</p> <p>If you encounter an issue that is not covered here, please ask your instructor for assistance.</p>"}]}