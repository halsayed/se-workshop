


# Glossary

This glossary provides definitions for key terms used throughout the workshop.

## A

**API (Application Programming Interface)**: A set of protocols and tools for building software applications. APIs specify how software components should interact.

**AI (Artificial Intelligence)**: The simulation of human intelligence in machines that are programmed to think and learn like humans.

## C

**CNCF (Cloud Native Computing Foundation)**: An open-source software foundation that promotes the adoption of cloud-native computing.

**Container**: A lightweight, standalone, executable package that includes everything needed to run an application.

## D

**Docker**: A platform for developing, shipping, and running applications using containerization technology.

## E

**Embedding**: A numerical representation of words, phrases, or documents that captures their semantic meaning.

**Endpoint**: A specific URL where an API can be accessed by a client application.

## F

**Fine-tuning**: The process of adapting a pre-trained model to a specific task by training it on a smaller, task-specific dataset.

**Flowise**: An open-source, low-code tool for building LLM orchestration flows and AI agents.

## G

**GenAI (Generative AI)**: A type of artificial intelligence that can generate new content, such as text, images, or code.

**GPU (Graphics Processing Unit)**: A specialized electronic circuit designed to rapidly manipulate and alter memory to accelerate the creation of images in a frame buffer intended for output to a display device.

## H

**Helm**: A package manager for Kubernetes that helps you manage Kubernetes applications.

**HPOC (Hosted POC)**: A Nutanix cluster specifically configured for high-performance storage workloads.

**Hugging Face**: A platform for sharing and discovering machine learning models, particularly focused on natural language processing.

## I

**Inference**: The process of using a trained model to make predictions on new data.

## K

**Kubernetes**: An open-source container orchestration platform for automating deployment, scaling, and management of containerized applications.

## L

**LangChain**: A framework for developing applications powered by language models.

**LLM (Large Language Model)**: A type of artificial intelligence model that is trained on large amounts of text data to understand and generate human-like text.

**LlamaIndex**: A data framework for LLM applications that helps connect custom data sources to large language models.

## N

**NAI (Nutanix Enterprise AI)**: Nutanix's enterprise-grade AI platform for deploying and managing AI workloads.

**NKP (Nutanix Kubernetes Platform)**: Nutanix's enterprise Kubernetes distribution.

**n8n**: An open-source workflow automation tool that allows you to connect different applications and services.

## P

**PEFT (Parameter-Efficient Fine-Tuning)**: A technique for fine-tuning large models by only updating a small number of parameters.

**Prompt Engineering**: The practice of designing and optimizing prompts to get better results from language models.

## Q

**Quantization**: A technique for reducing the precision of a model's weights to reduce memory usage and improve inference speed.

## R

**RAG (Retrieval-Augmented Generation)**: A technique that combines retrieval of relevant documents with generation to improve the accuracy and relevance of AI responses.

## T

**Token**: The basic unit of text that a language model processes. A token can be a word, part of a word, or a punctuation mark.

**Transformer**: A neural network architecture that is the foundation of most modern language models.

## V

**Vector Database**: A database optimized for storing and querying high-dimensional vectors, commonly used in AI applications for similarity search.

**VRAM (Video Random Access Memory)**: Memory used by graphics cards to store image data and other graphics-related information.

